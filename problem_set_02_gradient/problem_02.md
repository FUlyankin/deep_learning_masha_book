# 2. Логистическая регрессия

Маша решила, что нет смысла останавливаться на обычной регрессии, когда она знает, что есть ещё и логистическая:

\begin{equation*}
	\begin{aligned}
	& z  = w \cdot x \qquad p = \mathbb{P}(y = 1) = \frac{1}{1 + e^{-z}} \\
	& logloss = -[ y \cdot \ln p + (1 - y) \cdot \ln (1 - p) ]
	\end{aligned}
\end{equation*}

Целевая переменная, $y$ принимает значения $0$ и $1$. Аналитического решения в общем виде для такой задачи не существует. Приходится искать его с помощью градиентного спуска.  


__а)__ Запишите формулу, по которой можно пересчитывать веса в ходе градиентного спуска для логистической регрессии.

```{dropdown} Решение
Нам нужно взять производную от логистической функции потерь, заменим в ней $p$ на сигмоиду

$$
logloss = -1 \left (y \cdot \ln \left( \frac{1}{1 + e^{-z}} \right)  + (1 - y) \cdot \ln \left ( 1 - \frac{1}{1 + e^{-z}} \right ) \right)
$$

Теперь подставим вместо $z$ уравнение регрессии:

$$
logloss = -1 \left (y \cdot \ln \left( \frac{1}{1 + e^{-w \cdot x}} \right)  + (1 - y) \cdot \ln \left ( 1 - \frac{1}{1 + e^{- w \cdot x}} \right ) \right)
$$

Это и есть наша функция потерь.  От неё нам нужно найти производную. Давайте подготовимся. 

**Делай раз,** найдём производную $logloss$ по $p$: 

$$
logloss'_{p} = -1 \left(y \cdot \frac{1}{p} - (1 - y) \cdot \frac{1}{(1 - p)} \right)
$$

**Делай два,** найдём производную $\frac{1}{1 + e^{-w x}} $ по $w$:

\begin{multline*}
\left(  \frac{1}{1 + e^{-w x}}   \right)'_{w}  = - \frac{1}{(1 + e^{-w x})^2} \cdot e^{-w x} \cdot (-x) =\frac{1}{1 + e^{-w x}}  \cdot \frac{e^{-w x}}{1 + e^{-w x}} \cdot x  = \\ = \frac{1}{1 + e^{-w x}}  \cdot  \left(1 - \frac{1}{1 + e^{-w x}}  \right) \cdot x
\end{multline*}

По-другому это можно записать как $p \cdot (1 - p) \cdot x$.  

**Делай три,** находим полную производную:

\begin{multline*}
logloss'_{w} = -1 \left(y \cdot \frac{1}{p}  \cdot  p \cdot  \left(1 - p)  \right) \cdot x  - (1 - y) \cdot \frac{1}{(1 - p)} \cdot  p \cdot  \left(1 - p)  \right) \cdot x \right) = \\ =  -y \cdot \left( 1 - p \right) \cdot x + (1 - y) \cdot  p  \cdot x =  (-y + y p  + p - y p ) \cdot x = (p - y) \cdot x
\end{multline*}
```

__б)__ Оказалось, что $x = -5$, а $y = 1$. Сделайте один шаг градиентного спуска, если $w_0 = 1$, а скорость обучения $\gamma = 0.01$. 

```{dropdown} Решение
Найдём значение производной в точке $w_0 = 1$ для нашего наблюдения $x = -5, y=1$: 

$$
\left(\frac{1}{1 + e^{-1 \cdot (-5)}}  - 1 \right) \cdot (-5)  \approx  4.96
$$

Делаем шаг градиентного спуска: 

$$
w_1 = 1 - 0.01 \cdot 4.96 \approx 0.95
$$	
```

__в)__ Есть ли при решении этой задачи проблемы с локальными оптимумами? Если запустить повторно из другой инициализации, может получиться другой результат? Если да, то как с этим бороться?

```{dropdown} Решение
Проблемы с локальными оптимумами при оптимизации нет, так как функция потерь выпуклая.
```

