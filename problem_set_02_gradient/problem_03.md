# 3. Вопросики

Любой последователь Машиного лёрнинга должен знать ответы на вопросы ниже. Убедитесь, что вы их знаете. 

__а)__ Как вы думаете, почему считается, что SGD лучше работает для оптимизации функций, имеющих больше одного экстремума?

```{dropdown} Решение
Стохастический градиентный спуск работает довольно шумно. Из-за этого он может выпрыгивать из локальных минимумов и двигаться в сторону глобального минимума. 

```

__б)__ Предположим, что у функции потерь есть несколько локальных минимумов. Как можно адаптировать градиентный спуск так, чтобы он находил глобальный минимум чаще?

```{dropdown} Решение
Можно запустить градиентный спуск из разных начальных точек. Тогда мы придём в разные локальные минимумы. Среди них можно будет выбрать самый хороший. Такой приём называется **мультистарт.**

```

__в)__ Что будет происходить со стохастическим градиентным спуском, если длина его шага не будет уменьшаться от итерации к итерации? 

```{dropdown} Решение
Скорее всего, в окрестности точки оптимума, мы начнём просто блуждать вокруг неё и не сможем к ней приблизиться с нужной нам погрешностью. Сходимость стохастического градиентного спуска для выпуклых функций гарантируется только при уменьшающейся величине шага.

```
