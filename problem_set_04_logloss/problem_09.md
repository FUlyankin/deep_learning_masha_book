# 9. Предсказание вероятностей*

Маша хочет классифицировать пиво на правильное и неправильное. В её распоряжении есть выборка $(y_i, x_i)$. Переменная $y_i$ принимает значение $1$, если пиво правильное и значение $0$, если пиво неправильное. 

Выборка уже собрана, исследовательский энтузиазм зашкаливает. Есть только одна беда. Непонятно какую именно функцию потерь лучше использовать. 

Маше очень хотелось бы получить на выходе оценку вероятности принадлежности пива к определённому классу. Какую из функций лучше использовать?[^ml01]

__а)__ $L(y, a(x)) = (y - a(x))^2$

```{dropdown} Решение
Когда мы обучаем модель, мы считаем ошибку на обучающей выборке и минимизируем её по параметрам модели $a(x)$

$$
\frac{1}{n} \sum_{i=1}^{n} L(y_i, a(x_i)) \to \min_{w}
$$

В зависимости от того, какая выборка оказалась у нас в руках, могут получаться разные оценки параметров модели. При разных обучающих выборках, эмпирические потери могут принимать разные значения при одних и тех же параметрах модели. Эмпирическая функция потерь --- это случайная величина. На самом деле, нам хотелось бы оптимизировать математическое ожидание ошибки

$$
\mathbb{E}(L(y, a(x)) \mid x).
$$

Мы его не знаем, поэтому для его оценки мы используем эмпирические потери. Почему можно это делать? Эмпирические потери --- это выборочное среднее. По закону больших чисел оно должно при больших значениях $n$ сходиться к математическому ожиданию.

Ошибка на обучающей выборке $\frac{1}{n} \cdot \sum_{i=1}^{n} L(y_i, a(x_i))$ --- это эмпирическая оценка ожидаемых потерь $\mathbb{E}(L(y, a(x)) \mid x)$. Этот факт позволяет по-новому взглянуть на старые функции потерь. Минимизируя $\mathbb{E}(L(y, a) \mid x)$ по прогнозам, $a$, можно понять, что именно прогнозирует наш алгоритм. 

Будем считать, что для данных задана вероятностная модель $p(y \mid x)$. Посмотрим на то, где мы достигаем оптимума с точки зреня математического ожидания.

$$
\mathbb{E} \left[ L(y, a) \mid x \right] = \mathbb{P}(y = 1 \mid x) \cdot (a - 1)^2 + (1 - \mathbb{P}(y = 1 \mid x))(a - 0)^2.
$$

Для удобства обозначим $p = \mathbb{P}(y = 1 \mid x)$. Продифференцируем по $a$:

\begin{multline*}
\frac{\partial}{\partial a} \mathbb{E} \left[ L(y, a) \mid x \right] = 2 p (a - 1) + 2 (1 - p) a  = 2 \cdot a - 2 \cdot p = 0.
\end{multline*}
	
Решаем уравнение и получаем, что  оптимальный ответ алгоритма равен вероятности 

$$
a = p = \mathbb{P}(y = 1 \mid x).
$$
	
```

__б)__  $L(y, b(x)) = \mid y - a(x) \mid$

```{dropdown} Решение
Запишем математическое ожидание функции потерь

$$
\mathbb{E} \left[ L(y, a)|x\right] = p \cdot \mid 1 - a \mid + (1 - p) \cdot \mid a \mid = p \cdot (1 - a) + (1 - p) \cdot a.
$$

Перед нами линейная функция. Найти её оптимум с помощью взятия производной не выйдет, так как решение будет краевым. Немного перепишем выражение и проанализируем его

$$
\mathbb{E} \left[ L(y, a)|x\right] = p + (1 - 2p) \cdot a
$$

Нам нужно взять $a$ таким образом, чтобы минимизировать математическое ожидание. Прогноз ограничен отрезком $[0;1]$.

-  Если $1 - 2p > 0$, надо брать $a = 0$. Это занулит лишнее положительное слагаемое.
- Если $1 - 2p < 0$, надо брать $a = 1$. Так мы максимизируем отрицательное слагаемое.
- Если $1 - 2p = 0$, то есть $p=0.5$, нам безразлично какое $a$ брать.

Такие прогнозы не позволяют нам предсказать корректную вероятность в точке $x$. 

```

__в)__  $L(y, b(x)) = y \cdot \ln a(x) + (1 - y) \cdot \ln (1 - a(x))$

```{dropdown} Решение
Если $y=1$, останется только первое слагаемое. Если $y = 0$, останется только второе слагаемое. Это происходит с вероятностью $\mathbb{P}(y = 1 \mid x).$ Для удобства обозначим $p = \mathbb{P}(y = 1 \mid x)$ и запишем распределение функции потерь. 

|$L(y, a)$| $\ln a$ | $\ln(1 -a)$ |
|:-------:|:---:|:------:|
|$\mathbb{P}(L(y,a) = k)$|$p$|$1-p$|

Запишем математическое ожидание функции потерь

$$
\mathbb{E} \left[ L(y, a) \mid x \right] = p \cdot \ln a + (1 - p) \cdot \ln (1 - a).
$$

Продифференцируем по $a$

$$
\frac{p}{a} - \frac{(1 - p)}{(1 - a)} = 0.
$$


Решаем уравнение и получаем, что $a = p = \mathbb{P}(y = 1 \mid x).$

```

__г)__  $L(y, b(x)) = y \cdot a(x) + (1 - y) \cdot (1 - a(x))$

```{dropdown} Решение
Выпишем распределение функции потерь

|$L(y, a)$| $a$ | $1 -a$ |
|:-------:|:---:|:------:|
|$\mathbb{P}(L(y,a) = k)$|$p$|$1-p$|

Запишем математическое ожидание функции потерь

$$
\mathbb{E} \left[ L(y, a) \mid x \right] = p \cdot a + (1 - p) \cdot (1 - a).
$$

Это линейная функция. У нас не получится найти её оптимум взяв производную. Немного перепишем математическое ожидание

$$
\mathbb{E} \left[ L(y, a) \mid x \right]  = a \cdot (2p - 1) + (1 - p) \to \min_{a}.
$$

Мы можем влиять только на выбор $a$. Нам надо его выбрать так, чтобы минимизировать выражение.

- Если $(2p - 1) > 0$, надо выбрать $a = 0$.
- Если $(2p - 1) < 0,$ надо выбрать $a = 1$. 
- Если $(2p - 1) = 0$, то есть $p = 0.5$, нам безразлично каким выбрать $a$. 

Алгоритм не будет прогнозировать вероятности.

```

[^ml01]: Задачка скопирована и немного дополнена из [семинара по МО-1 с ФКН](https://github.com/esokolov/ml-course-hse/blob/master/2022-fall/seminars/sem06-probs-quantile.pdf)
[^shad]: Также более подробно про калибровку можно почитать в [учебнике по МО от ШАД](https://ml-handbook.ru/chapters/prob_calibration/intro)



