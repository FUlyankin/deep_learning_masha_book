# 1. Сигмоида

Сигмоида -- это классическая функция активации. У неё есть куча проблем, из-за которых её нужно очень аккуратно использовать в глубоких нейронных сетях. Давайте обсудим эти проблемы и поймём как правильно её использовать.


Любую $s$-образную функцию называют сигмоидой. Наиболее сильно прославилась под таким названием функция 

$$
f(t) = \frac{e^t}{1 + e^t}.
$$ 

Слава о ней добралась до Маши и теперь она хочет немного поисследовать её свойства.[^mynotebb]


__а)__ Что происходит при $t \to +\infty$? А при $t \to -\infty$?

```{dropdown} Решение

```

__б)__ Как связаны между собой $f(t)$ и  $f(-t)$?

```{dropdown} Решение

```

__в)__ Как связаны между собой $f'(t)$ и  $f'(-t)$?

```{dropdown} Решение

```

__г)__ Как связаны между собой $f(t)$ и $f'(t)$? 

```{dropdown} Решение

```

__д)__ Найдите $f(0)$, $f'(0)$ и $\ln f(0)$.

```{dropdown} Решение

```

__е)__ Найдите обратную функцию $f^{-1}(t)$

```{dropdown} Решение

```

__ё)__ Как связаны между собой $\frac{d\ln f(t)}{dt}$ и $f(-t)$?

```{dropdown} Решение

```

__ж)__ Постройте графики функций $f(t)$ и $f'(t)$.

```{dropdown} Решение

```

__з)__ Говорят, что сигмоида --- это гладкий аналог единичной ступеньки. Попробуйте построить на компьютере графики $f(t), f(10\cdot t), f(100\cdot t), f(1000\cdot t)$. Как они себя ведут?

```{dropdown} Решение

```

__и)__ Выпишите формулы для forward pass и backward pass через слой с сигмоидой.

```{dropdown} Решение

```

__к)__ Какое максимальное значение принимает производная сигмоиды? Объясните как это способствует затуханию градиента и параличу нейронной сети?

```{dropdown} Решение

```

__л)__ Сигмоида не центрированна относительно нуля. Из-за этого градиентный спуск работает плохо. Объясните, почему так происходит. 

```{dropdown} Решение

```

[^mynotebb]: Задачка взята из [коллекции Бориса Демешева](https://github.com/bdemeshev/mlearn_pro) и немного доработана.
