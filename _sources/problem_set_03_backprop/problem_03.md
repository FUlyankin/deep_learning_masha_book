# 3. Сигмоида

В **не**глубоких сетях в качестве функции активации можно использовать сигмоиду

$$
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{1 + e^{z}},
$$

Маша хочет использовать сигмоиду внутри нейросети. Предполагается, что после прямого шага, наши вычисления будут использованы в другой части нейросети. В конечном итоге, по выходу из нейросети, мы вычислим какую-то функцию потерь $L$. 

У сигмоиды нет параметров. Чтобы обучить нейросеть, Маше понадобится производная $\frac{\partial L}{\partial z}$. Выпишите её в матричном виде через производные $\frac{\partial L}{\partial \sigma}$ и $\frac{\partial \sigma}{\partial z}$.


```{dropdown} Решение
При решении предыдущей задачи мы выяснили, что $\sigma'(z) = \sigma(z) \cdot (1 - \sigma(z)).$ Тогда по цепному правилу 

$$
\frac{\partial L}{\partial z}  = \frac{\partial L}{\partial \sigma} \cdot \frac{\partial \sigma }{\partial z}  =  \frac{\partial L}{\partial \sigma} \cdot \sigma(z) \cdot (1 - \sigma(z)).
$$

Получается, при прямом проходе мы вычисляем сигмоиду по формуле из условия. При обратном проходе мы умножаем пришедшую к нам производную на производную сигмоиды. Если на вход приходит матрица, мы берём сигмоиду от каждого её элемента. Если на вход приходит матрица $Z_{[n \times k]},$ на выходе мы получаем матрицу $\Sigma_{[n \times k]}.$

Когда мы берём сигмоиду от матрицы, мы применяем функцию к каждому её элементу. из-за этого, в производной все умножения мы делаем поэлементно, то есть матрица $\Sigma * (1 - \Sigma)$ останется размера $[n \times k].$  

Когда мы применяем цепное правило, под $\frac{\partial L}{\partial \Sigma}$ мы подразумеваем производную функции потерь $L$ по каждому элементу матрицы $\Sigma$. Получается, что это матрица размера $[n \times k].$ Её мы поэлементно умножаем на $\Sigma * (1 - \Sigma)$ и снова получаем матрицу размера $[n \times k].$ Все размерности оказываются соблюдены.
```