# 8. Нестеров и бэкпроп

К Маше приехал её папа и загрузил её интересным вопросом. В алгоритме обратного распространения ошибки мы можем делать шаг как минимум двумя способами: 

1. Зафиксировали все $w_{t-1},$ нашли все градиенты, сделали сразу по всем весам шаг градиентного спуска.
2. Нашли градиенты для последнего слоя и сделали шаг для его весов, получили $w_t^k.$ Для поиска градиентов предпоследнего слоя используем веса  $w_t^k,$ а не $w_{t-1}^k.$ Все остальные слои обновляем по аналогии. 

Как думаете, какой из способов будет приводить к более быстрой сходимости и почему?

```{note}
Я придумал эту задачу и не смог найти статью, где делали бы что-то похожее. Если вы видели такую, пришлите её мне. Мои контакты есть во введении к книге. Ниже пара моих мыслей о том, что может происходить на практике при таком подходе. Эксперименты я не ставил, хотя хотел. 
```

```{dropdown} Мысли автора
С одной стороны идея чем-то похожа на градиентный спуск с поправкой Нестерова. Возможно, сходимость ускорится. С другой стороны, градиенты оказываются смещёнными. Если сеть глубокая, в её начале смещение может быть очень большим. Из-за этого сходимость может сломаться. 
```