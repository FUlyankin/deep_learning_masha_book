# 6. Температура генерации

Иногда в функцию $\text{softmax}$ добавляют дополнительный параметр $T$, который называют **температурой сэмплирования.** Тогда она приобретает вид 

$$ 
\frac{e^{\tfrac{z_i}{T}}}{ \sum_{k=1}^K e^{\tfrac{z_k}{T}}}
$$

Обычно это делается, когда с помощью нейросетки нужно сгенерировать какой-нибудь новый объект. Пусть у нас есть три класса. Наша нейросеть выдала на последнем слое числа $1,2,5$[^nik_note].

__а)__ Какое итоговое распределение вероятностей мы получим, если $T = 10$? А если $T = 1$? А если $T = 0.1$? 

```{dropdown} Решение
Подставим числа в формулу:

- если $T = 10,$ получаем $p \approx (0.39, 0.36, 0.25)$;
- если $T = 1,$ получаем $p \approx (0.72, 0.27, 0.01)$;
- если $T = 0.1$ получаем $p \approx (0.9999, 4\cdot 10^{-5}, 4 \cdot 10^{-18})$.

Чем меньше $T,$ тем ярче выражен максимум. Чем больше $T$, тем мы ближе к равномерному распределению. 

```

__б)__ Какое распределение получится при $T \to 0$? А при $T \to \infty$? 

```{dropdown} Решение
Если $T \to \infty$, тогда $e^{\tfrac{z_i}{T}} \to 1$. Получется, что в нашем векторе все три числа при бесконечном $T$ примут значение $1/3,$ то есть мы получим равномерное распределение. 

Если $T \to 0,$ мы получим распределение полностью сосредоточенное в исходе с максимальным весом. Это видно из численного примера из пункта __а).__

```

__в)__ Предположим, что объектов на порядок больше. Например, это реплики, которые Алиса может сказать вам в ответ на какую-то фразу.  Понятное дело, что вашей фразе будет релевантно какое-то подмножество ответов. Какое значение температуры сэмплирования $T$ смогут сделать реплики Алисы непредсказуемыми? А какие сделают их однотипными? 

```{dropdown} Решение
Маленькие значения $T$ сделают Алису однотипной, а большие разнообразной. Понятное дело, что если распределение окажется близким к равномерному, Алиса будет нести несвязную чушь. Параметр $T$ аккуратно подбирают после обучения нейронной сетки. 

При од­них и тех же весах модели сэмплирование с высокой температурой будет более разнообразным, в том числе часто совершенно случайным, а сэмплирование с низкой температурой будет более устойчивым, чаще будет выдавать одно и то же при похожих входах.

```


[^nik_note]: По мотивам книги Николенко "Глубокое обучение" (стр. 266-267)
