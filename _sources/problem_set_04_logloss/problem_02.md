# 2. Логистические потери и softmax


У Маши три наблюдения, первое наблюдение --- кит, остальные --- муравьи. Киты кодируются $y_i = 1$, муравьи --- $y_i = 0$.  В качестве регрессоров Маша берёт номера наблюдений $x_i = i$. После этого Маша оценивает логистическую регрессию с константой. В качестве функции потерь используются логистические потери.

__а)__ Выпишите для данной задачи функцию потерь, которую минимизирует Маша.

```{dropdown} Решение

```

__б)__ При каких оценках коэффициентов логистической регрессии эта функция достигает своего минимума?

**Hint:** Изобразите наблюдения на числовой прямой и подумайте как должна будет вести себя идеальная сигмоида. Обратите внимание, что выборка линейно-разделима. 

```{dropdown} Решение

```

__в)__ Маша чуть внимательнее присмотрелась к своему третьему наблюдению и поняла, что это не муравей, а бобёр. Теперь ей нужно решать задачу классификации на три класса. Она решила использовать для этого нейросеть с softmax-слоем на выходе. 

```{dropdown} Решение

```

__г)__ Маша уже обучила нейронную сетку и хочет построить прогнозы для двух наблюдений. Слой, который находится перед $\softmax$ выдал для этих двух наблюдений следующий результат: $(1, -2, 0)$ и $(0.5, -1, 0)$.

Чему равны вероятности получить кита, муравья и бобра для этих двух наблюдений? 

```{dropdown} Решение

```

__д)__ Пусть первым был кит, а вторым бобёр.  Чему будет равна $logloss$-ошибка? 

```{dropdown} Решение

```

__е)__  Пусть у Маши есть два класса. Она хочет выучить нейросеть. Она может учить нейронку с одним выходом и сигмоидой в качестве функции активации либо нейронку с двумя выходами и $softmax$ в качестве функции активации. Как выходы этих двух нейронок взаимосвязаны между собой? 

```{dropdown} Решение

```

__ё)__ Объясните, почему $softmax$  считают сглаженным вариантом arg max.

```{dropdown} Решение

```

__ж)__ Докажите, что $softmax(z + c) = softmax(z)$, где $c$ --- какая-то константа, прибавленная ко всем выходам слоя. Как этот факт позволяет сделать $softmax$ численно устойчивой функцией и упростить для компьютера оптимизацию нейросети? 

```{dropdown} Решение
При поиске softmax мы ищем экспоненты, в памяти компьютера может произойти переполнение из-за больших чисел. Возникнут бесконечности и оптимизация нейронной сетки умрёт.

Если добавить ко всем входам нейронки одинаковую константу, значение sofrmax не изменится: 

$$
\frac{e^{z_i + c}}{\sum_{k=1}^K e^{z_k + c}} = \frac{ e^c \cdot e^{z_i } }{e^c \cdot \sum_{k=1}^K e^{z_k} }   = 	\frac{e^{z_i}}{\sum_{k=1}^K e^{z_k }} 
$$

Из-за этого, в нейросетях считают устойчивый к переполнению softmax: 

$$
softmax(z_1, \ldots, z_{K}) = softmax(z_1 - \max_i (z_i), \ldots, z_{K} - \max_i (z_i))
$$
```











