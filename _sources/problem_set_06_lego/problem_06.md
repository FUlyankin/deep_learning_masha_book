# 6. Инициализация весов 

::::{important}
Надо доделать задачу про часть с ReLU
::::

Маша использует для активации симметричную функцию. Для инициализации весов она хочет использовать распределение 

$$
w_i \sim U \left[ - \frac{1}{\sqrt{n_{in}}};  \frac{1}{\sqrt{n_{in}}}  \right].
$$

__а)__ Покажите, что это будет приводить к затуханию дисперсии при переходе от одного слоя к другому. 

```{dropdown} Решение
Найдём дисперсию веса 

$$
Var(w_i) = \frac{1}{12} \cdot \left( \frac{1}{\sqrt{n_{in}} + \frac{1}{\sqrt{n_{in}}}} \right)^2 = \frac{1}{3 \cdot n_{in}} 
$$

Мы используем линейный слой с симметричной функцией активации. Мы можем сгенеировать веса так, чтобы они были независимы друг от друга и от наблюдений. Наблюдения считаем независимыми друг от друга. Тогда 

\begin{multline*}
Var(h_i) = Var \left(\sum_{i = 1}^{n_{in}} w_i x_i \right) = \\ = \sum_{i = 1}^{n_{in}} Var(w_i \cdot x_i) = \\ =  \sum_{i = 1}^{n_{in}} \mathbb{E}^2(x_i) \cdot Var(w_i) + Var(x_i) \cdot \mathbb{E}^2(w_i) + Var(x_i) \cdot Var(w_i).
\end{multline*}

Мы воспользовались тут формулой для [дисперсии произведения независимых случайных величин.](https://en.wikipedia.org/wiki/Variance#Product_of_independent_variables)

Наша функции активации симметрична, значит $\mathbb{E}(x_i) = 0$. Будем инициализировать веса из распределения с нулевым средним, тогда $\mathbb{E}(w_i) = 0.$ Воспользуемся тем, что все слагаемые распределены одинаково, а их у нас $n_{in}$ штук

$$
Var(h_i) = \sum_{i = 1}^{n_{in}} Var(x_i) \cdot Var(w_i) = n_{in} \cdot Var(x_i) \cdot Var(w_i).
$$

Получается, что 

$$
Var(h_i) = n_{in} \cdot Var(x_i) \frac{1}{3 \cdot n_{in}}  = \frac{1}{3} \cdot Var(x_i).
$$

Дисперсия будет падать при переходе от выхода предыдущего слоя к выходу нового слоя в три раза. 

```

__б)__ Какими нужно взять параметры равномерного распределения, чтобы дисперсия не затухала? 

```{dropdown} Решение
Нам надо, чтобы $Var(h_i) = Var(x_i).$ Значит надо взять распределение с дисперсией $\frac{1}{n_{in}}$. 

Если речь идёт про нормальное распределение, нам подойдёт

$$
w_i \sim U \left[ - \frac{\sqrt{3}}{\sqrt{n_{in}}};  \frac{\sqrt{3}}{\sqrt{n_{in}}}  \right].
$$

```

__в)__ Маша хочет инициализировать веса из нормального распределения. Какими нужно взять параметры, чтобы дисперсия не затухала? 

```{dropdown} Решение
По аналогии с предыдущим пунктом, нам нужна инициализация 

$$
w_i \sim N \left(0, \frac{1}{n_{in}} \right).
$$

```

__г)__ При прямом распространении ошибки на вход в нейрон идёт $n_{in}$ слагаемых. При обратном распространении ошибки на вход в нейрон идёт $n_{out}$ градиентов. 

Количество весов от слоя к слою сильно колеблется, получается если мы будем пытаться делать дисперсию неизменной при прямом шаге, она будет либо расти либо падать при обратном. Невозможно пооддерживать обе дисперсии неизменными. Нужен компромис между этими шагами. Предложите его. 


```{dropdown} Решение
При прямом шаге надо поддерживать дисперсию $\frac{1}{n_{in}}$. При обратном надо поддерживать дисперсию $\frac{1}{n_{out}}$. 

Можно инициализировать веса из распределения с дисперсией $\frac{2}{n_{in} + n_{out}}$. Такая инициализация назвывается **инициализацией Ксавие (или Глорота).**[^statia_xav] 

```

<!-- Внутри нейрона в качестве функции активации используется ReLU. На вход идёт $10$ признаков. В качестве инициализации для весов используется нормальное распределение, $N(0,1)$. С какой вероятностью нейрон будет выдавать на выход нулевое наблюдение, если 

Предположения на входы? Какое распределение и с какими параметрами надо использовать, чтобы этого не произошло? Сюда же про инициализацию Хе.

__г)__

```{dropdown} Решение

```

__д)__ 

```{dropdown} Решение

```
 -->
[^statia_xav]: [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)


