# 6. Инициализация весов 

Маша использует для активации симметричную функцию. Для инициализации весов она хочет использовать распределение 

$$
w_i \sim U \left[ - \frac{1}{\sqrt{n_{in}}};  \frac{1}{\sqrt{n_{in}}}  \right].
$$

__а)__ Покажите, что это будет приводить к затуханию дисперсии при переходе от одного слоя к другому. 

```{dropdown} Решение

```

__б)__ Какими нужно взять параметры равномерного распределения, чтобы дисперсия не затухала? 

```{dropdown} Решение

```

__в)__ Маша хочет инициализировать веса из нормального распределения. Какими нужно взять параметры, чтобы дисперсия не затухала? 

```{dropdown} Решение

```

Внутри нейрона в качестве функции активации используется ReLU. На вход идёт $10$ признаков. В качестве инициализации для весов используется нормальное распределение, $N(0,1)$. С какой вероятностью нейрон будет выдавать на выход нулевое наблюдение, если 

Предположения на входы? Какое распределение и с какими параметрами надо использовать, чтобы этого не произошло? Сюда же про инициализацию Хе.

__г)__

```{dropdown} Решение

```

__д)__ 

```{dropdown} Решение

```

