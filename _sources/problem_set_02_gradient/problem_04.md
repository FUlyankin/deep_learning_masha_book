# 4. Квадратное уравнение

Маша вместе с директором отдела по искусственному интеллекту Теслы Андреем Карпати[^tesla]. ищет минимум функции $f(x) = ax^2 + bx +c,$ где $a > 0,$ методом градиентного спуска. Они стартуют из точки $x_0$ и настолько ленивы, что не хотят делать больше одного шага. При каком значении длины шага $\eta$ ребята за один шаг окажутся точно в точке минимума? 


```{dropdown} Решение
За один шаг нам надо попасть в вершину параболы, то есть 

\begin{equation*}
    \begin{aligned}
        & \frac{-b}{2a} = x_0 - \eta \cdot f'(x_0) \\
        & \frac{-b}{2a} = x_0 - \eta \cdot (2 a x_0 + b) \\
        & \frac{b + 2 a x_0}{2a} = \eta \cdot (2 a x_0 + b) \\
        & \eta = \frac{1}{2a} \\
    \end{aligned} 
\end{equation*} 

О чём нам говорит это упражнение? Скорость обучения -- очень важна. Если подобрать её правильно, мы можем довольно быстро оказаться в точке оптимума. Когда мы оптимизируем нейронные сети, скорость обучения важно подбирать. Для этого даже есть специальные утилиты, например LRFinder.
```


[^tesla]: Бывшим директором. У Андрея есть очень [классный блог.](https://karpathy.ai/) Более того, он один из авторов [курса по нейросетям cs231n,](https://cs231n.github.io/) который читается в Стэнфорде. 