# 2. Регуляризация

::::{important}
Задачка не доделана. Надо доделать оставшиеся пункты, добавить сюда приём от Хидмана с CV-оценкой в Ridge за одну регрессию.
::::


Маша ест конфеты и решает задачи по глубокому обучению. Число решённых задач $y$ зависит от числа съеденных конфет $x$. Если Маша ни съела ни одной конфеты, она не хочет заниматься глубоким обучением. Поэтому для прогнозирования числа решённых задач по числу съеденных конфет можно использовать линейную модель с одним признаком без константы $y_i = w \cdot x_i.$

Для оценки параметра $w$ Маша использует целевую функцию

$$
Q(w) = \frac{1}{n}\sum_{i=1}^{n} (y_i - w x_i)^2 + \lambda w^2 \to \min_{w}.
$$

__а)__ Найдите оптимальное $w$ при произвольном $\lambda$.


```{dropdown} Решение
Перед нами линейная модель с $L_2$ регуляризатором. Возьмём производную

\begin{equation*}
	\begin{aligned}
		& Q'(w) =  \frac{1}{n}\sum_{i=1}^{n} -2 \cdot (y_i - w x_i) \cdot x_i + 2 \lambda w = 0 \\
		& \sum_{i=1}^{n} y_i x_i - w \sum_{i=1}^{n} x_i^2 - n \lambda w = 0 \\
		& \sum_{i=1}^{n} y_i x_i = w \cdot (\sum_{i=1}^{n} x_i^2 + n \lambda) \\
		& \hat w = \frac{\sum_{i=1}^{n} y_i x_i}{\sum_{i=1}^{n} x_i^2 + n \lambda }
	\end{aligned}
\end{equation*}

Чем больше значение $\lambda$, тем ближе к нулю значение коэффициента. Регуляризация стягивает коэффициенты к нулю. Когда модель слишком сильно подстраивается под данные, коэффициенты в ней оказываются очень большими. Стягивание к нулю запрещает модели слишком сильно подстраиваться под данные. 

```

__б)__ Подберите оптимальное $\lambda$ с помощью кросс-валидации leave one out («выкинь одного»). На первом шаге мы оцениваем модель на всей выборке без первого наблюдения, а на первом тестируем её. На втором шаге мы оцениваем модель на всей выборке без второго наблюдения, а на втором тестируем её. И так далее $n$ раз. Чтобы найти $\lambda_{CV}$ мы минимизируем среднюю ошибку, допущенную на тестовых выборках.


```{dropdown} Решение

```

__в)__ Найдите оптимальное значение $w$ при $\lambda_{CV}$, подобранном на предыдущем шаге. 


```{dropdown} Решение

```

__г)__ Обычно, чтобы сделать loo-кросс-валидацию, мы оцениваем $n-1$ модель, где $n$ -- число наблюдений. Для Ridge-регрессии можно сделать кросс-валидацию, оценив модель только один раз. Сможете придумать как? 


```{dropdown} Решение

```

__д)__ Иногда при конструировании нейронной сети, накладывают регуляризацию на выход из нейрона. Как думаете, зачем это делают? 


```{dropdown} Решение

```


