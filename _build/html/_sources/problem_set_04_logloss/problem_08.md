# 8. Модернизируем logloss

__а)__  Маша построила для своей выбороки распределение таргета, чтобы понять насколько оно сбалансировано


```{figure} ../images/problem_set_04/img08_balanced_logloss.png
---
width: 50%
name: logloss_weight
---
```

Обычно Маша минимизировала такой $logloss$

$$
- \frac{1}{n} \sum_{i=1}^n (y_i \cdot \ln p_i + (1 - y_i) \cdot \ln( 1 - p_i)).
$$

В этот раз Маша решила минимизировать немного модернизированную функцию

$$
- \frac{1}{n} \sum_{i=1}^n ({\color{red} 3 \cdot} y_i \cdot \ln p_i + {\color{red} 1 \cdot} (1 - y_i) \cdot \ln( 1 - p_i)) 
$$

Как думаете, зачем Маша сделал это?

```{dropdown} Решение
Маша увидела, что в выборке есть серьёзный дисбаланс. Первый класс встречается в три раза реже нулевого. Из-за этого Мааш решила искусственно увеличить значение каждого слагаемого с $\ln p_i$ для первого класса в три раза. 

Если хочется сделать такое в \textbf{sklearn}, можно внутри \textbf{LogisticRegression} поставить \textbf{weighted = 'balanced'}. Тогда он сам рассчитает насколько велик дисбаланс в тренировочной выборке и увеличит одно из слагаемых. 

Если целевой класс встречается редко, наша модель может довольно сильно деградировать и начать везде выдавать нули. 
```

__б)__ Маша прочитала 

Маша прочитала в статье про функцию потерь Focal Loss.[^fl_note] Её обычно используют при решении задачи детекции изображений. 

Важной частью всех детекторов изображений является решение задачи классификации для большого количества участков изображения. При этом многие из этих участков (патчей) содержат только фон, то есть преобаладает класс "фон". Многие из изображений фона легко классифицируются как фон. 

То есть возникает много примеров одного из классов, которые при этом и так легко классифицируются моделью. Из-за этого при использовании логистической функции поетрь, мы неэффективно учимся. Введем обозначение

$$
p_t = \begin{cases} p, & \mbox{if } y = 1 \\ 1 - p, & \mbox{if } y = 0 \end{cases}
$$

В логистической функции потерь мы находим штраф по формуле 

$$
CE(p_t) = - \ln (p_t),
$$

в случае focal loss штраф считается по формуле

$$
FL(p_t) = - \alpha_t (1 - p_t)^\gamma \ln p_t.
$$

Как думаете, за что отвечают параметры $\alpha$ и $\gamma$? Как они помогают решить проблему преобладания класса с фоном? 


```{dropdown} Решение
Классическую логистическую функцию потерь в задаче бинарной классификации можно записать следующим образом:

$$
CE(y, p) = \begin{cases} - \ln (p), & \mbox{if }  y = 1 \\ - \ln (1-p), & \mbox{if } y = 0. \end{cases}
$$

Можно записать функцию потерь через $p_t$ как 

$$
CE(y, p) = CE(p_t) = - \ln (p_t)
$$

В случае, когда $p_t \gg 0.5$ объект классифицируется правильно, ошибка хоть и мала, но всё ещё ненулевая, а в случае большого количества простых для классификации примеров преобладающий класс может "задавить" более редкий в функции потерь. Модель будет учиться для этих объектов предсказывать всё более близкие к 1 вероятности вместо того, чтобы лучше классифицировать объекты более редкого класса.

Одним из вариантов борьбы с эффектом несбалансированных классов является добавление балансирующего коэффициента. В таком случае $CE(p_t) = - \alpha_t \ln p_t$, где

$$
\alpha_t = \begin{cases} \alpha, & \mbox{if } y = 1 \\ 1 - \alpha, & \mbox{if } y = 0 \end{cases}
$$

Значение $\alpha \in (0, 1).$ Обычно этот коэффициент пропорционален балансу классов и, таким образом, в функции потерь объекты редкого класса начинают "весить" столько же, сколько и объекты популярного класса.

Однако в этом случае всё ещё "лёгкие" для модели объекты из-за своего количества могут перебивать "сложные" примеры. Можно модифицировать исходную формулу добавлением коэффициента, зависящего от предсказаний:

$$
FL(p_t) = - (1 - p_t)^\gamma \log p_t, \gamma \in [0, 5]
$$.

При $\gamma = 0$ это всё ещё стандартная логистическая функция потерь, а для других значений получаем, что вес объекта зависит от того, насколько модель уверена в своих предсказаниях. В таком случае при обучении модель будет меньше стремиться "дотюниваться" под популярные и "лёгкие" примеры. 

Также можно использовать коэффициент для баланса между классами: $FL(p_t) = - \alpha_t (1 - p_t)^\gamma \ln p_t$.

На картинке представлен вид focal loss для различных значений параметра $\gamma$ и логистическая функция потерь ($\gamma = 0$). Можно увидеть, что в правой части графика, когда модель уверена в предсказаниях, штраф тем меньше, чем больше $\gamma$.

<img src="../images/problem_set_04/img08_focal_loss.png" alt="dobronet_forward" width="95%" align="center">

```

[^fl_note]: [Focal Loss for Dense Object Detection](https://arxiv.org/pdf/1708.02002.pdf)
[^fc_sol]: Решение задачи про Focal Loss я взял из [семинара по МО-2 с ФКН](https://github.com/esokolov/ml-course-hse/blob/master/2021-spring/seminars/sem21-multilabel.pdf)



