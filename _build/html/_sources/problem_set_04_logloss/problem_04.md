# 4. Гиперболический тангенс

Функция $f(t) = \tanh(t) = \frac{2}{1 + e^{-2t}} - 1$ называется гиперболическим тангенсом.


__а)__ Что происходит при $t \to +\infty$? А при $t \to -\infty$?

```{dropdown} Решение


Мы знаем, что при $t \to +\infty$ функция $e^{-2t} \to 0$, значит $f(t) \to 1.$ 

По аналогии при $t \to -\infty$ функция $e^{-2t} \to \infty$, значит $f(t) \to -1.$ 

```

__б)__ Как связаны между собой $f(t)$ и $f'(t)$? Как они выглядят на графике? 

```{dropdown} Решение
Если взять производную сложной функции, можно получить, что 

$$
f'(t) = 1 - f(t)^2.
$$

Изобразим функцию активации и её производную на графике 

<img src="../images/problem_set_04/img04_tah.png" alt="dobronet_forward" width="95%" align="center">

```

__в)__ Выпишите формулы для forward pass и backward pass через слой с тангенсом. 

```{dropdown} Решение
Мы уже нашли выше производную. У гиперболического тангенса нет обучаемых параметров, получается прямой проход через неё делается по формуле

$$
o = f(h).
$$

Обратный шаг делается по формуле

$$
d = (1 - f(h)^2) \cdot d.
$$

То же самое можно записать как 

$$
\frac{\partial L}{\partial h} = (1 - f(h)^2) \cdot \frac{\partial L}{\partial o}.
$$

```

__г)__ Правда ли, что тангенс способствует затуханию градиента и параличу нейронной сети? Какое максимальное значение принимает производная тангенса? 

```{dropdown} Решение
Да, правда. Значение 1 - $f(t)^2$ лежит между $0$ и $1$. 
```

__д)__ Как у тангенса дела с центированием относительно нуля? 

```{dropdown} Решение
В отличие от сигмоиды, тангенс центрирован относительно нуля. Пожалуй, это его единственный плюс. В остальном, это плохая функция активации, способствующая затуханию градиента.
```

__е)__ Сигмоиду обычно используют для того, чтобы на последнем слое получать вероятности. Либо в местах, где внутри нейросети нужно отщипнуть долю от какого-нибудь числа (например, в LSTM). Как думаете, где на практике используют тангенс? 

```{dropdown} Решение
Тангенс не используют в качестве функции активации из-за тех же самых недостатков, что и сигмоиду. Обычно его используют, когда на выходе из слоя нужно получить число с отрезка $[-1; 1]$.

Например, когда работают с картинками, их нормируют на отрезок $[-1; 1].$ Если мы захотим обучить нейросетку, которая генерирует картинки, на выходе нам надо будет получать числа с этого отрезка. Тут нам и пригодится тангенс. 
```
